# 3. Rancher 클러스터 구축

이제 가장 길고 중요한 과정을 진행하는 단계에 접어들었습니다.

## 3.1. 계속하기 전에

Cloud Shell의 `~/clouddrive` 디렉터리와 로컬 작업 환경에 모두 Git 리포지터리를 체크아웃했는지 다시 한 번 확인하여 주십시오.

## 3.2. 스크립트 파일 작성하기

아래의 스크립트의 내용을 하나씩 입력하면서 진행할 것입니다. 창 분할 기능을 이용하여 브라우저와 JetBrains IDE를 좌우로 띄워놓고 실습을 진행하도록 하겠습니다. Terraform 플러그인의 기능을 활용하여 잘못 타이핑하거나 오류가 있는 내용은 교정하면서 진행할 수 있습니다.

### main.tf 파일

```hcl
variable arm-prefix {
  default = "rkttu-rke"
}

variable dns-prefix {
  default = "rktturke"
}

variable vm-names {
  type = "list"
  default = [
    "controlplane",
    "etcd",
    "worker1",
    "worker2",
    "worker3"
  ]
}

variable vm-roles {
  type = "list"
  default = [
    "controlplane",
    "etcd",
    "worker",
    "worker",
    "worker"
  ]
}

variable vm-admin-username {
  default = "rkttu"
}

variable vm-admin-password {
  default = "rhksflwk949!"
}

data "template_file" "docker-deploy-script" {
  template = "${file("docker-deploy-script.tpl")}"
  vars {
    vm_admin_username = "${var.vm-admin-username}"
  }
}

# *********************** RKE CLUSTER *********************** #
resource "azurerm_resource_group" "rke-group" {
  name = "${var.arm-prefix}-group"
  location = "japaneast"
}

# ********************** VNET / SUBNET ********************** #
resource "azurerm_virtual_network" "rke-vnet" {
  name = "${var.arm-prefix}-vnet"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  location = "${azurerm_resource_group.rke-group.location}"
  address_space = ["10.0.0.0/16"]
}

resource "azurerm_subnet" "rke-subnet" {
  name = "${var.arm-prefix}-subnet"
  virtual_network_name = "${azurerm_virtual_network.rke-vnet.name}"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  network_security_group_id = "${azurerm_network_security_group.rke-secgroup.id}"
  address_prefix = "10.0.1.0/24"
  depends_on = ["azurerm_virtual_network.rke-vnet"]
}

# **********************  STORAGE ACCOUNTS ********************** #
resource "azurerm_storage_account" "rke-storage" {
  name = "rkestore"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  location = "${azurerm_resource_group.rke-group.location}"
  account_tier = "Standard"
  account_replication_type = "LRS"
}

# **********************  NETWORK SECURITY GROUP ********************** #
resource "azurerm_network_security_group" "rke-secgroup" {
  name = "${var.arm-prefix}-secgroup"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  location = "${azurerm_resource_group.rke-group.location}"

  security_rule {
    name = "allow-ssh"
    description = "Secure Shell"
    priority = 100
    direction = "Inbound"
    access = "Allow"
    protocol = "Tcp"
    source_port_range = "*"
    destination_port_range = "22"
    source_address_prefix = "*"
    destination_address_prefix = "*"
  }

  security_rule {
    name = "rke-cluster-1"
    description = "Kubernetes Communication Port 1"
    priority = 110
    direction = "Inbound"
    access = "Allow"
    protocol = "Tcp"
    source_port_range = "*"
    destination_port_range = "6443"
    source_address_prefix = "*"
    destination_address_prefix = "*"
  }

  security_rule {
    name = "rke-cluster-2"
    description = "Kubernetes Communication Port 2"
    priority = 120
    direction = "Inbound"
    access = "Allow"
    protocol = "Tcp"
    source_port_range = "*"
    destination_port_range = "2379"
    source_address_prefix = "*"
    destination_address_prefix = "*"
  }

  security_rule {
    name = "rke-cluster-3"
    description = "Kubernetes Communication Port 3"
    priority = 130
    direction = "Inbound"
    access = "Allow"
    protocol = "Tcp"
    source_port_range = "*"
    destination_port_range = "2380"
    source_address_prefix = "*"
    destination_address_prefix = "*"
  }
}

# **********************  PUBLIC IP ADDRESSES ********************** #
resource "azurerm_public_ip" "rke-public-ip" {
  name = "${var.arm-prefix}-public-ip"
  location = "${azurerm_resource_group.rke-group.location}"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  public_ip_address_allocation = "Static"
  domain_name_label = "${var.dns-prefix}"
}

# **********************  AVAILABILITY SET ********************** #
resource "azurerm_availability_set" "rke-availability-set" {
  name = "${var.arm-prefix}-set"
  location = "${azurerm_resource_group.rke-group.location}"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  managed = true
  
  # region 별로 지정 가능한 fault domain의 숫자가 다릅니다. 아래 리소스를 참조하세요.
  # https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/managed-disks-common-fault-domain-region-list.md
  platform_fault_domain_count = 2
}

# **********************  NETWORK INTERFACES ********************** #
resource "azurerm_network_interface" "rke-nic" {
  name = "${var.arm-prefix}-${element(var.vm-names, count.index)}"
  location = "${azurerm_resource_group.rke-group.location}"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  network_security_group_id = "${azurerm_network_security_group.rke-secgroup.id}"
  count = "${length(var.vm-names)}"
  depends_on = ["azurerm_virtual_network.rke-vnet", "azurerm_public_ip.rke-public-ip", "azurerm_lb.rke-load-balancer"]

  ip_configuration {
    name = "ipconfig${count.index}"
    subnet_id = "${azurerm_subnet.rke-subnet.id}"
    private_ip_address_allocation = "Static"
    private_ip_address = "10.0.1.${count.index + 4}"
    load_balancer_backend_address_pools_ids = ["${azurerm_lb_backend_address_pool.rke-backend-pool.id}"]

    load_balancer_inbound_nat_rules_ids = [
      "${element(azurerm_lb_nat_rule.rke-ssh-inbound-rule.*.id, count.index)}",
      "${element(azurerm_lb_nat_rule.rke-http-inbound-rule.*.id, count.index)}",
      "${element(azurerm_lb_nat_rule.rke-https-inbound-rule.*.id, count.index)}",
    ]
  }
}

# **********************  LOAD BALANCER ********************** #
resource "azurerm_lb" "rke-load-balancer" {
  name = "${var.arm-prefix}-load-balancer"
  location = "${azurerm_resource_group.rke-group.location}"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  depends_on = ["azurerm_public_ip.rke-public-ip"]

  frontend_ip_configuration {
    name = "${var.arm-prefix}-ssh-ip-config"
    public_ip_address_id = "${azurerm_public_ip.rke-public-ip.id}"
  }
}

resource "azurerm_lb_backend_address_pool" "rke-backend-pool" {
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  loadbalancer_id = "${azurerm_lb.rke-load-balancer.id}"
  name = "${var.arm-prefix}-backend-pool"
}

# **********************  LOAD BALANCER INBOUND NAT RULES ********************** #
resource "azurerm_lb_nat_rule" "rke-ssh-inbound-rule" {
  name                           = "${var.arm-prefix}-ssh-inbound-${count.index}"
  resource_group_name            = "${azurerm_resource_group.rke-group.name}"
  loadbalancer_id                = "${azurerm_lb.rke-load-balancer.id}"
  protocol                       = "tcp"
  frontend_port                  = "6400${count.index + 1}"
  backend_port                   = 22
  frontend_ip_configuration_name = "${var.arm-prefix}-ssh-ip-config"
  count                          = "${length(var.vm-names)}"
  depends_on                     = ["azurerm_lb.rke-load-balancer"]
}

resource "azurerm_lb_nat_rule" "rke-http-inbound-rule" {
  name                           = "${var.arm-prefix}-http-inbound-${count.index}"
  resource_group_name            = "${azurerm_resource_group.rke-group.name}"
  loadbalancer_id                = "${azurerm_lb.rke-load-balancer.id}"
  protocol                       = "tcp"
  frontend_port                  = "8${count.index + 0}"
  backend_port                   = 3306
  frontend_ip_configuration_name = "${var.arm-prefix}-ssh-ip-config"
  count                          = "${length(var.vm-names)}"
  depends_on                     = ["azurerm_lb.rke-load-balancer"]
}

resource "azurerm_lb_nat_rule" "rke-https-inbound-rule" {
  name                           = "${var.arm-prefix}-https-inbound-${count.index}"
  resource_group_name            = "${azurerm_resource_group.rke-group.name}"
  loadbalancer_id                = "${azurerm_lb.rke-load-balancer.id}"
  protocol                       = "tcp"
  frontend_port                  = "44${count.index + 3}"
  backend_port                   = 9200
  frontend_ip_configuration_name = "${var.arm-prefix}-ssh-ip-config"
  count                          = "${length(var.vm-names)}"
  depends_on                     = ["azurerm_lb.rke-load-balancer"]
}

# ********************** VIRTUAL MACHINES ********************** #
resource "azurerm_virtual_machine" "rke-node-vm" {
  name                  = "${element(var.vm-names, count.index)}"
  resource_group_name   = "${azurerm_resource_group.rke-group.name}"
  location              = "${azurerm_resource_group.rke-group.location}"
  vm_size               = "Standard_DS1_v2"
  network_interface_ids = ["${element(azurerm_network_interface.rke-nic.*.id, count.index)}"]
  count                 = "${length(var.vm-names)}"
  availability_set_id   = "${azurerm_availability_set.rke-availability-set.id}"
  depends_on            = ["azurerm_availability_set.rke-availability-set", "azurerm_network_interface.rke-nic", "azurerm_storage_account.rke-storage"]

  storage_os_disk {
    name = "${element(var.vm-names, count.index)}-os-disk"
    caching = "ReadWrite"
    create_option = "FromImage"
    managed_disk_type = "Premium_LRS"
    disk_size_gb = "128"
  }

  storage_image_reference {
    publisher = "Canonical"
    offer = "UbuntuServer"
    sku = "16.04.0-LTS"
    version = "latest"
  }

  os_profile {
    computer_name = "${element(var.vm-names, count.index)}"
    admin_username = "${var.vm-admin-username}"
    admin_password = "${var.vm-admin-password}"
  }

  os_profile_linux_config {
    disable_password_authentication = false
  }

  boot_diagnostics {
    enabled = "true"
    storage_uri = "${azurerm_storage_account.rke-storage.primary_blob_endpoint}"
  }
}

resource "azurerm_virtual_machine_extension" "rke-setup-script" {
  name = "${var.arm-prefix}-${count.index}-setup-script"
  resource_group_name = "${azurerm_resource_group.rke-group.name}"
  location = "${azurerm_resource_group.rke-group.location}"
  virtual_machine_name = "${element(azurerm_virtual_machine.rke-node-vm.*.name, count.index)}"
  publisher = "Microsoft.Azure.Extensions"
  type = "CustomScript"
  type_handler_version = "2.0"
  auto_upgrade_minor_version = true
  count = "${length(var.vm-names)}"
  depends_on = ["azurerm_virtual_machine.rke-node-vm", "azurerm_lb_nat_rule.rke-ssh-inbound-rule"]

  settings = <<SETTINGS
{
  "script": "${base64encode(data.template_file.docker-deploy-script.rendered)}"
}
SETTINGS
}

# *********************** OUTPUT *********************** #

#output "endpoint-ip-address" {
#  value = "${azurerm_public_ip.rke-public-ip.ip_address}"
#}

#output "endpoint-ssh-ports" {
#  value = "${azurerm_lb_nat_rule.rke-ssh-inbound-rule.*.frontend_port}"
#}

output "sample" {
  value = <<EOF

---
nodes:
${join("\n", formatlist("  - address: ${azurerm_public_ip.rke-public-ip.ip_address}\n    user: ${var.vm-admin-username}\n    port: %s\n    role: [%s]", "${azurerm_lb_nat_rule.rke-ssh-inbound-rule.*.frontend_port}", "${var.vm-roles}"))}
services:
  etcd:
    image: quay.io/coreos/etcd:latest
  kube-api:
    image: rancher/k8s:v1.8.3-rancher2
  kube-controller:
    image: rancher/k8s:v1.8.3-rancher2
  scheduler:
    image: rancher/k8s:v1.8.3-rancher2
  kubelet:
    image: rancher/k8s:v1.8.3-rancher2
  kubeproxy:
    image: rancher/k8s:v1.8.3-rancher2
network:
  plugin: flannel
addons: |-
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-nginx
      namespace: default
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
EOF
}
```

### docker-deploy-script.tpl 파일

```sh
#!/bin/bash

# Install docker daemon
curl https://releases.rancher.com/install-docker/17.03.sh | sh

# Add VM admin user to docker group
usermod -aG docker ${vm_admin_username}
```

### up.sh 파일

```sh
#!/bin/bash
wget https://github.com/rancher/rke/releases/download/v0.1.1/rke_linux-amd64
chmod +x ./rke_linux-amd64
./rke_linux-amd64 up
```

### .gitignore 파일

```text
# Compiled files
*.tfstate
*.tfstate.backup

# Module directory
.terraform/

# IDEA related directory
.idea/

# cluster.yml file (Output)
cluster.yml
```

위의 파일들을 git 리포지터리에 추가한 다음, 원격 리포지터리에 Push하고, Cloud Shell 상의 `~/clouddrive` 디렉터리에 있는 리포지터리에 Pull합니다.

만약 이 과정이 기억나지 않으신다면 모듈 2로 돌아가 내용을 다시 확인하여 주십시오.

Cloud Shell에 변경된 파일을 모두 전달하였으면, 아래 순서대로 명령을 Cloud Shell 환경에서 입력합니다.

1. `terraform plan` 을 입력합니다.

2. `terraform apply` 를 입력합니다. 실행을 묻는 프롬프트에서 `yes`를 입력하고, 모든 작업이 완료될 때까지 기다립니다.

3. 처리가 완료되면 출력 데이터들이 나타나는데, 출력 데이터에 RKE 설치 도구를 위한 스크립트가 자동으로 완성되어 나타납니다.

4. 마우스로 텍스트 영역을 드래그하면 블록을 선택할 수 있으며, Ctrl + C 키는 터미널로 입력이 들어갈 수 있으므로, 마우스 오른쪽 버튼을 클릭하여 복사합니다. 만약 내용을 놓쳤다면, `terraform apply` 명령을 다시 입력하면 인프라 상에 변경된 내용이 없으므로 곧바로 출력 결과를 다시 확인할 수 있습니다.

5. 메모장에 복사한 데이터를 Git 리포지터리에 `cluster.yml` 파일로 저장하고, 같은 과정을 거쳐 원격 리포지터리와 Cloud Shell 상의 리포지터리에 추가합니다.

이제 RKE를 실행하기 위한 준비가 완료되었습니다.

## 3.3. RKE 설치 프로그램 사용하기

위의 파일들을 이용하여 RKE 설치 프로그램을 Cloud Shell 환경에서 다운로드하고 설치를 진행하겠습니다.

1. Cloud Shell의 Git 리포지터리로 이동하여 `chmod +x up.sh` 명령을 입력하여 스크립트에 대한 실행 권한을 부여하도록 합니다.

2. `./up.sh` 명령을 실행하여 스크립트를 실행합니다.

3. RKE가 각 서버의 SSH 세션에 접속하면서 Kubernetes 클러스터 구축 작업을 자동으로 진행합니다.

4. 모든 설치가 끝나면 `.kube_config_cluster.yml` 파일이 작업 디렉터리에 만들어집니다. 이 파일을 `~/.kube/config` 파일로 복사합니다.

5. `kubectl get nodes` 명령을 입력하여 각 노드들이 정상적으로 실행되었는지 확인합니다.

RKE의 자세한 사용법과 설명에 대한 내용은 [An Introduction To RKE](http://rancher.com/an-introduction-to-rke/) 문서를 참고하시기 바랍니다.

마지막으로 백엔드와 프론트엔드로 구성된 실제 예제 애플리케이션을 배포해보도록 하겠습니다.
